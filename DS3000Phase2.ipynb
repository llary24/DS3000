{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35935a5c-af0e-4781-9ae6-808ce1c99b44",
   "metadata": {},
   "source": [
    "# Phase II: Data Curation, Exploratory Analysis and Plotting (5\\%)\n",
    "\n",
    "### Team Members:\n",
    "- Logan Lary\n",
    "- Mark Tran\n",
    "- Sabrina Valerjev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c048d52-5297-4694-a9a4-5134bf78fba1",
   "metadata": {},
   "source": [
    "## Part 1: \n",
    "(1%) Expresses the central motivation of the project and explains the (at least) two key questions to be explored. Gives a summary of the data processing pipeline so a technical expert can easily follow along."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b496568b-b26e-439b-8490-b9b92e4d1dab",
   "metadata": {},
   "source": [
    "## Project Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b1f408-fa48-4b88-9f8d-3a0acc3176e9",
   "metadata": {},
   "source": [
    "## Summary of the Data Processing Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da990590-da74-45a0-b145-a2ed093ef27d",
   "metadata": {},
   "source": [
    "## Part 2: \n",
    "(2\\%) Obtains, cleans, and merges all data sources involved in the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "27158c4c-68f4-450a-af25-e0872d72041b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding relevant imports\n",
    "import requests\n",
    "from requests_html import HTML\n",
    "import json\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "import requests\n",
    "from requests_html import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b4e939c3-1542-4f20-b4fb-0854bf130785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source 1: Box Office Mojo\n",
    "@dataclass\n",
    "class ScrapeBoxOffice:\n",
    "    base_endpoint:str = \"https://www.boxofficemojo.com/year/world/\"\n",
    "    year:int = None\n",
    "    save_raw:bool = False\n",
    "    save:bool = False\n",
    "    output_dir: str = \".\"\n",
    "    table_selector: str = '.imdb-scroll-table'\n",
    "    table_data = []\n",
    "    table_header_names = []\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    @property\n",
    "    def name(self):\n",
    "        return self.year if isinstance(self.year, int) else 'world'\n",
    "    \n",
    "    def get_endpoint(self):\n",
    "        endpoint = self.base_endpoint\n",
    "        if isinstance(self.year, int):\n",
    "            endpoint = f\"{endpoint}{self.year}/\"\n",
    "        return endpoint\n",
    "    \n",
    "    def get_output_dir(self):\n",
    "        return pathlib.Path(self.output_dir)\n",
    "    \n",
    "    def extract_html_str(self, endpoint=None):\n",
    "        url = endpoint if endpoint is not None else self.get_endpoint()\n",
    "        r = requests.get(url, stream=True)\n",
    "        html_text = None\n",
    "        status = r.status_code\n",
    "        if r.status_code == 200:\n",
    "            html_text = r.text\n",
    "            if self.save_raw:\n",
    "                output_fname = f\"{self.name}.html\"\n",
    "                raw_output_dir = self.get_output_dir() / 'html'\n",
    "                raw_output_dir.mkdir(exist_ok=True, parents=True)\n",
    "                output_fname = raw_output_dir / output_fname\n",
    "                with open(f\"{output_fname}\", 'w') as f:\n",
    "                    f.write(html_text)\n",
    "            return html_text, status\n",
    "        return html_text, status\n",
    "    \n",
    "    def parse_html(self, html_str=''):\n",
    "        r_html = HTML(html=html_str)\n",
    "        r_table = r_html.find(self.table_selector)\n",
    "        if len(r_table) == 0:\n",
    "            return None\n",
    "        table_data = []\n",
    "        header_names = []\n",
    "        parsed_table = r_table[0]\n",
    "        rows = parsed_table.find(\"tr\")\n",
    "        header_row = rows[0]\n",
    "        header_cols = header_row.find('th')\n",
    "        header_names = [x.text for x in header_cols]\n",
    "        for row in rows[1:]:\n",
    "            cols = row.find(\"td\")\n",
    "            row_data = []\n",
    "            row_dict_data = {}\n",
    "            for i, col in enumerate(cols):\n",
    "                header_name = header_names[i]\n",
    "                row_data.append(col.text)\n",
    "            table_data.append(row_data)\n",
    "        self.table_data = table_data\n",
    "        self.table_header_names = header_names\n",
    "        return self.table_data, self.table_header_names\n",
    "    \n",
    "    def to_df(self, data=[], columns=[]):\n",
    "        return pd.DataFrame(data, columns=columns)\n",
    "    \n",
    "    def run(self, save=False):\n",
    "        save = self.save if save is False else save\n",
    "        endpoint = self.get_endpoint()\n",
    "        html_str, status = self.extract_html_str(endpoint=endpoint)\n",
    "        if status not in range(200, 299):\n",
    "            raise Exception(f\"Extraction failed, endpoint status {status} at {endpoint}\")\n",
    "        data, headers = self.parse_html(html_str if html_str is not None else '')\n",
    "        df = self.to_df(data=data, columns=headers)\n",
    "        self.df = df\n",
    "        if save:\n",
    "            filepath = self.get_output_dir() / f'{self.name}.csv'\n",
    "            df.to_csv(filepath, index=False)\n",
    "        return self.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "db54ad64-51c1-424f-a23d-073a58994715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source 2: OMDb\n",
    "API_KEY = \"f3eb77a3\"\n",
    "URL = \"http://www.omdbapi.com/?t=\"\n",
    "\n",
    "def get_movie_data(url, movie):\n",
    "    ''' Takes in the name of a movie and returns associated data on the movie.'''\n",
    "    movie_link = process_movie_name(movie)\n",
    "    complete_url = url + movie_link + \"&apikey=\" + API_KEY\n",
    "    response = requests.get(complete_url) \n",
    "    return response.json()\n",
    "\n",
    "def process_movie_name(movie):\n",
    "    ''' Takes in the name of a movie and modifies it so that it can be used in API call.'''\n",
    "    words = movie.split()\n",
    "    return '+'.join(words)\n",
    "\n",
    "# get the list of all movies in a year\n",
    "# get data on all those movies\n",
    "# save to a json\n",
    "def get_year_movie_data(movie_titles, url, year):\n",
    "    empty_data = {}\n",
    "    data_list = []\n",
    "    for movie in movie_titles:\n",
    "        response = get_movie_data(url, movie)\n",
    "        data_list.append(response)\n",
    "    with open(\"MovieData\" + year + \".json\", 'w') as json_file:\n",
    "        json.dump(data_list, json_file, indent=4) \n",
    "\n",
    "# year = 2010\n",
    "# while year < 2024:\n",
    "#     scrapper = ScrapeBoxOffice(year=year, save=True, save_raw=True, output_dir='data')\n",
    "#     df_box = scrapper.run()\n",
    "#     movies_year = df_box[\"Release Group\"].tolist()\n",
    "#     MovieDataCollection.get_year_movie_data(movies_year, \"http://www.omdbapi.com/?t=\", str(year))\n",
    "#     year = year + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f970b0-47a7-4a26-a419-ce43b9092a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape data for all years\n",
    "scraper_2011 = ScrapeBoxOffice(year=2011, save=True, save_raw=True, output_dir='data')\n",
    "df_2011 = scraper_2011.run()\n",
    "\n",
    "# scraper_2012 = ScrapeBoxOffice(year=2012, save=True, save_raw=True, output_dir='data')\n",
    "# df_2012 = scraper_2012.run()\n",
    "\n",
    "# scraper_2013 = ScrapeBoxOffice(year=2013, save=True, save_raw=True, output_dir='data')\n",
    "# df_2013 = scraper_2013.run()\n",
    "\n",
    "# scraper_2014 = ScrapeBoxOffice(year=2014, save=True, save_raw=True, output_dir='data')\n",
    "# df_2014 = scraper_2014.run()\n",
    "\n",
    "# scraper_2015 = ScrapeBoxOffice(year=2015, save=True, save_raw=True, output_dir='data')\n",
    "# df_2015 = scraper_2015.run()\n",
    "\n",
    "# scraper_2016 = ScrapeBoxOffice(year=2016, save=True, save_raw=True, output_dir='data')\n",
    "# df_2016 = scraper_2016.run()\n",
    "\n",
    "# scraper_2017 = ScrapeBoxOffice(year=2017, save=True, save_raw=True, output_dir='data')\n",
    "# df_2017 = scraper_2017.run()\n",
    "\n",
    "# scraper_2018 = ScrapeBoxOffice(year=2018, save=True, save_raw=True, output_dir='data')\n",
    "# df_2018 = scraper_2018.run()\n",
    "\n",
    "# scraper_2019 = ScrapeBoxOffice(year=2019, save=True, save_raw=True, output_dir='data')\n",
    "# df_2019 = scraper_2019.run()\n",
    "\n",
    "# scraper_2020 = ScrapeBoxOffice(year=2020, save=True, save_raw=True, output_dir='data')\n",
    "# df_2020 = scraper_2020.run()\n",
    "\n",
    "# scraper_2021 = ScrapeBoxOffice(year=2021, save=True, save_raw=True, output_dir='data')\n",
    "# df_2021 = scraper_2021.run()\n",
    "\n",
    "# scraper_2022 = ScrapeBoxOffice(year=2022, save=True, save_raw=True, output_dir='data')\n",
    "# df_2022 = scraper_2022.run()\n",
    "\n",
    "# scraper_2023 = ScrapeBoxOffice(year=2023, save=True, save_raw=True, output_dir='data')\n",
    "# df_2023 = scraper_2023.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0e8dc4c1-2369-4d13-8b1b-ad41661375bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge box office data and movie data\n",
    "def merge_data(year, box_df):\n",
    "    '''Merges box office data and movie data.'''\n",
    "    file_path_movie = \"MovieData\" + year + \".json\"\n",
    "    df_movie_data = pd.read_json(file_path_movie)\n",
    "    box_df_bet = box_df.rename(columns={\"Release Group\": 'Title'})\n",
    "    master_df = pd.merge(df_movie_data, box_df_bet, on = \"Title\", how = \"inner\")\n",
    "    master_df[\"Year\"] = year\n",
    "    return master_df\n",
    "\n",
    "# master_2010 = merge_data(\"2010\", df_2010)\n",
    "# master_2011 = merge_data(\"2011\", df_2011)\n",
    "# master_2012 = merge_data(\"2012\", df_2012)\n",
    "# master_2013 = merge_data(\"2013\", df_2013)\n",
    "# master_2014 = merge_data(\"2014\", df_2014)\n",
    "# master_2015 = merge_data(\"2015\", df_2015)\n",
    "# master_2016 = merge_data(\"2016\", df_2016)\n",
    "# master_2017 = merge_data(\"2017\", df_2017)\n",
    "# master_2018 = merge_data(\"2018\", df_2018)\n",
    "# master_2019 = merge_data(\"2019\", df_2019)\n",
    "# master_2020 = merge_data(\"2020\", df_2020)\n",
    "# master_2021 = merge_data(\"2021\", df_2021)\n",
    "# master_2022 = merge_data(\"2022\", df_2022)\n",
    "# master_2023 = merge_data(\"2023\", df_2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30ab22d-2a0b-4d75-99cc-b0c3d815cd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating one big dataframe\n",
    "# dataframes = [master_2010, master_2011, master_2012, master_2013, master_2014, master_2015, master_2016, master_2017,\n",
    "#               master_2018, master_2019, master_2020, master_2021, master_2022, master_2023]\n",
    "\n",
    "def merge_dataframes(dfs):\n",
    "    return pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# all_data = merge_dataframes(dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb529a3-b064-4b6d-8ffc-05cbf2e0667b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning the data\n",
    "def clean_box_office(df):\n",
    "    '''Cleans box office sales by removing dollar signs and commas, and drops rows where Domestic value is \"-\".'''\n",
    "    # Clean Worldwide column\n",
    "    df = df[df[\"Domestic\"] != \"-\"]\n",
    "    df = df.dropna(subset = [\"Worldwide\", \"Domestic\", \"Foreign\"])\n",
    "    df[\"Worldwide\"] = (\n",
    "        df[\"Worldwide\"]\n",
    "        .astype(str)  \n",
    "        .str.replace(\"$\", \"\", regex=False)  \n",
    "        .str.replace(\",\", \"\", regex=False)  \n",
    "        .astype(int)\n",
    "    )\n",
    "    # Clean Domestic column\n",
    "    df[\"Domestic\"] = (\n",
    "        df[\"Domestic\"]\n",
    "        .astype(str)  \n",
    "        .str.replace(\"$\", \"\", regex=False) \n",
    "        .str.replace(\",\", \"\", regex=False)  \n",
    "    \n",
    "    )\n",
    "    # Clean Foreign column\n",
    "    df[\"Foreign\"] = (\n",
    "        df[\"Foreign\"]\n",
    "        .astype(str)  \n",
    "        .str.replace(\"$\", \"\", regex=False)  \n",
    "        .str.replace(\",\", \"\", regex=False)  \n",
    "    )\n",
    "    # Creating new columns because the raw numbers are too large to process\n",
    "    df[\"Worldwide_millions\"] = pd.to_numeric(df[\"Worldwide\"]) / 1000000\n",
    "    df[\"Domestic_millions\"] = pd.to_numeric(df[\"Domestic\"]) / 1000000\n",
    "    df[\"Foreign_millions\"] = pd.to_numeric(df[\"Foreign\"], errors=\"coerce\") / 1000000\n",
    "    return df\n",
    "\n",
    "#cleaned_df = clean_box_office(all_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81dcbcad-e4da-40c2-b17f-e716616c6c3b",
   "metadata": {},
   "source": [
    "## Part 3:\n",
    "(2\\%) Builds at least two visualizations (graphs/plots) from the data which help to understand or answer the questions of interest. These visualizations will be graded based on how much information they can effectively communicate to readers. Please make sure your visualization are sufficiently distinct from each other."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
